{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Packages"
      ],
      "metadata": {
        "id": "lFAzRRODji-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai\n",
        "!pip install langchain\n",
        "!pip install unstructured\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "iZl_Pdvdxw6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip dir of text files\n",
        "!unzip \"/content/data.zip\""
      ],
      "metadata": {
        "id": "UYd2ft7C4joo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dj9KV0enwbAT"
      },
      "outputs": [],
      "source": [
        "# Inbuild packages\n",
        "import os\n",
        "from typing import List\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Third party packages\n",
        "from tqdm import tqdm\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI\n",
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader, DirectoryLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load All files and process files"
      ],
      "metadata": {
        "id": "h9SbnuXl4Ycx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load all files and into one document\n",
        "loader = DirectoryLoader('./data', glob = \"**/*.txt\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "OK-N9NoQxAOc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert text data into chunks"
      ],
      "metadata": {
        "id": "cQ7wYWYu51wA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting document into text\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 100, chunk_overlap = 50)\n",
        "texts = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "tmEdTi1k5DtT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Store all text data into database as embedding form"
      ],
      "metadata": {
        "id": "PYcX2UU0_NTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"db\"\n",
        "\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\", google_api_key = \"AIzaSyAJc5QKxVwHJiUl1V9jFiunoeUTW123FCM\")\n",
        "\n",
        "vector_db = Chroma.from_documents(documents = texts,\n",
        "                                  embedding = embedding_model,\n",
        "                                  persist_directory = persist_directory)"
      ],
      "metadata": {
        "id": "WU9XNIkc6cTp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# persiste the db to disk\n",
        "vector_db.persist()\n",
        "vector_db = None\n",
        "\n",
        "# Now we can load the persisted database from disk, and use it as normal.\n",
        "vector_db = Chroma(persist_directory = persist_directory,\n",
        "                   embedding_function = embedding_model)\n",
        "vector_db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSaDD5vE6cyk",
        "outputId": "33b386c3-fce1-4085-fe8b-2ecb3d3bb9ea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x78eeea520460>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retriver data from database"
      ],
      "metadata": {
        "id": "vPZPqc7FCmft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "c_DdpGzNB1ak"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Make a chain for Q & A"
      ],
      "metadata": {
        "id": "7NcHxAJkDRJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm = GoogleGenerativeAI(model = \"gemini-pro\", google_api_key = \"AIzaSyAJc5QKxVwHJiUl1V9jFiunoeUTW123FCM\"),\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "ATYlCJlNCtKR"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cite sources\n",
        "def process_llm_response(llm_response):\n",
        "    print(\"Answer : \\n\")\n",
        "    print(llm_response['result'])"
      ],
      "metadata": {
        "id": "ppi_nKCQEaXd"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full example --> There is not such data\n",
        "query = \"How much money did Pando raise?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaZvMC-qE74d",
        "outputId": "d2568c67-59cf-45b4-c8f7-d29d00ee5b15"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer : \n",
            "\n",
            "I don't know. The provided text does not mention how much money Pando raised.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# British TV viewers lead the trend of illegally downloading US shows from the net\n",
        "query = \"What is the news of Hotspot users gain free net calls?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDXDSAQSE9pq",
        "outputId": "11a7b8b8-c8ad-42ac-81d0-5ee047ec622a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer : \n",
            "\n",
            "People using wireless net hotspots will soon be able to make free phone calls as well as surf the internet.\n"
          ]
        }
      ]
    }
  ]
}